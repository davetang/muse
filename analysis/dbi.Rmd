---
title: "Interfacing with databases in R"
date: "`r Sys.Date()`"
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(DBI)
library(RSQLite)
library(dbplyr)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The [{DBI} package](https://cran.r-project.org/web/packages/DBI/index.html) provides:

> A database interface definition for communication between R and relational database management systems. All classes in this package are virtual and need to be extended by the various R/DBMS implementations.

A database table can be thought of as a data frame, however there are three high-level differences between them:

1. Database tables are stored on disk and can be arbitrarily large, whereas data frames are stored in memory and are fundamentally limited.

2. Database tables almost always have indexes making it possible to quickly find rows of interest without having to look at every single row. Data frames don't have indexes but data tables do, which is one of the reasons why they're so fast.

3. Most classical databases are optimised for rapidly collecting data and not for analysing existing data. These databases are called _row-oriented_ because the data is stored row by row, rather than column by column like data frames. More recently, there's been much development of _column-oriented_ databases that make analysing existing data much faster.

Databases are run by database management systems (DBMS), which come in three basic forms:

1. Client-server DBMS run on a powerful central server, which you connect from your computer (the client). They are useful for sharing data with multiple people and popular client-server DBMS include PostgreSQL, MariaDB, SQL Server, and Oracle.

2. Cloud DBMS, like Snowflake, Amazon's RedShift, and Google's BigQuery, are similar to client-server DBMS, but they run in the cloud, taking advantage of cloud capabilities.

3. _In-process_ DBMS, like SQLite or duckdb, run entirely on your computer. They're great for working with large datasets where you are the primary user.

## Connecting to a database

To connect to a database in R, we need:

1. The DBI package because it provides a set of generic functions that connect to the database.
2. A specific package tailored for the DBMS of interest; this package translates the generic DBI commands into the specifics.

The {RSQLite} package provides a SQLite interface for R.

> Embeds the SQLite database engine in R and provides an interface compliant with the DBI package. The source for the SQLite engine and for various extensions in a recent version is included. System libraries will never be consulted because this package relies on static linking for the plugins it includes; this also ensures a consistent experience across all installations.

SQLite database downloaded as per the post [Interfacing with the Sequence Read Archive in R](sradb.html).

```{r mydb}
dbfile <- "/data2/sradb/SRAmetadb.sqlite"
mydb <- dbConnect(RSQLite::SQLite(), dbname = dbfile)
mydb
```

## DBI basics

Lists all tables in the database.

```{r db_list_tables}
tabs <- dbListTables(mydb)
tabs
```

Neat trick to get the fields of a table.

```{r get_fields}
get_fields <- "SELECT * FROM run WHERE 1=0"
DBI::dbGetQuery(mydb, get_fields)
```

Save all table fields.

```{r table_fields}
table_fields <- purrr::map(tabs, \(x){
  sql <- paste0('SELECT * FROM ', x, ' WHERE 1=0')
  DBI::dbGetQuery(mydb, sql)
})

names(table_fields) <- tabs
table_fields[[1]]
```

Disconnect.

```{r disconnect}
DBI::dbDisconnect(mydb)
```
