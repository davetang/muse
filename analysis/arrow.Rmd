---
title: "Getting started with the R arrow package"
date: "`r Sys.Date()`"
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(arrow)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The [{arrow} package](https://cran.r-project.org/web/packages/arrow/index.html) provides an interface to the Arrow C++ library.

> [Apache Arrow](https://arrow.apache.org/) is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware.

An example file was downloaded using [curl](curl.html).

```{r multi_download}
outdir <- 'data'
library_file <- "seattle-library-checkouts.csv"
outfile <- paste0(outdir, '/', library_file)
stopifnot(file.exists(outfile))
```

File size.

```{r file_size}
file.size(outfile) |> utils:::format.object_size(units = 'Gb')
```

`arrow::open_dataset()` will scan a the input file and figure out the structure of the dataset; it will only read further rows if specified. Code below from a [GitHub issue](https://github.com/hadley/r4ds/issues/1374#issuecomment-1479893607).

```{r seattle_csv}
opts <- CsvConvertOptions$create(col_types = schema(ISBN = string())) 

seattle_csv <- open_dataset( 
  sources = "data/seattle-library-checkouts.csv",  
  format = "csv", 
  convert_options = opts 
) 
seattle_csv
```

Get a glimpse of the data.

```{r seattle_csv_glimpse}
seattle_csv |> dplyr::glimpse()
```

Use `collect()` to force arrow to perform a computation to return some data.

```{r seattle_csv_collect}
seattle_csv |>
  dplyr::count(CheckoutYear, wt = Checkouts) |>
  dplyr::arrange(CheckoutYear) |>
  dplyr::collect()
```

## The Parquet Format

The parquet format is used for rectangular data and is a custom binary format designed specifically for large datasets.

* Parquet files are usually smaller than the equivalent CSV file because of efficient encodings and file compression.
* Parquet files have a rich type system.
* Parquet files are "column-oriented", like R's data frame.
* Parquet files are chunked making it possible to work on different parts of the file at the same time.

Partition the Seattle library data by `CheckoutYear`, since it is likely some analyses will want to only look at recent data and partitioning by year yields 18 chunks of reasonable size.

```{r partition_seattle_csv_by_checkout_year}
pq_path <- 'data/seattle-library-checkouts'

seattle_csv |>
  dplyr::group_by(CheckoutYear) |>
  arrow::write_dataset(path = pq_path, format = "parquet")
```

Examine files.

```{r parquet_files}
tibble::tibble(
  files = list.files(pq_path, recursive = TRUE),
  size_MB = file.size(file.path(pq_path, files)) / 1024^2
)
```

### Using dplyr with Arrow

Open parquet files.

```{r seattle_pq}
seattle_pq <- open_dataset(pq_path)
```

Write a dplyr query.

```{r query}
seattle_pq |>
  dplyr::count(CheckoutYear, wt = Checkouts) |>
  dplyr::arrange(CheckoutYear) -> query
```

Collect.

```{r query_collect}
query |> dplyr::collect() |> system.time()
```

Compare runtime.

```{r compare_runtime}
seattle_csv |>
  dplyr::count(CheckoutYear, wt = Checkouts) |>
  dplyr::arrange(CheckoutYear) |>
  dplyr::collect() |>
  system.time()
```
