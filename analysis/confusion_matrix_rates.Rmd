---
title: "R function for calculating confusion matrix rates"
date: "`r Sys.Date()`"
output:
  workflowr::wflow_html:
    toc: true
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```

I often forget the names and aliases (and how to calculate them) of confusion matrix rates and have to look them up. Finally, I had enough and was looking for a single function that could calculate the most commonly used rates, like sensitivity or precision, but I couldn't find one that didn't require me to install some R package. Therefore I wrote my own called [table_metrics](https://github.com/davetang/learning_r/blob/main/code/table_metrics.R) and will briefly talk about it in this post.

I have had this [Simple guide to confusion matrix terminology](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) bookmarked for many years and I keep referring back to it. It does a great job of explaining the list of rates that are often calculated from a confusion matrix for a binary classifier. If you need a refresher on the confusion matrix rates/metrics, check it out.

We can generate the same confusion matrix as the Simple guide with the following code.

```{r}
generate_example <- function(){
  dat <- data.frame(
    n = 1:165,
    truth = c(rep("no", 60), rep("yes", 105)),
    pred = c(rep("no", 50), rep("yes", 10), rep("no", 5), rep("yes", 100))
  )
  table(dat$truth, dat$pred)
}

confusion <- generate_example()
confusion
```

I wrote the function [confusion_matrix()](https://github.com/davetang/learning_r/blob/main/code/confusion_matrix.R) to generate a confusion matrix based on case numbers. The same confusion matrix can be generated with the function by sourcing it from GitHub.

```{r}
source("https://raw.githubusercontent.com/davetang/learning_r/main/code/confusion_matrix.R")
eg <- confusion_matrix(TP=100, TN=50, FN=5, FP=10)
eg$cm
```

To use the table_metrics function I wrote, you also source it directly from GitHub.

```{r}
source("https://raw.githubusercontent.com/davetang/learning_r/main/code/table_metrics.R")
```

The function has four parameters, which are described below using roxygen2 syntax (copied and pasted from the source code of the table_metrics function).

```
#' @param tab Confusion matrix of class table
#' @param pos Name of the positive label
#' @param neg Name of the negative label
#' @param truth Where the truth/known set is stored, `row` or `col`
```

To use `table_metrics()` on the example data we generated, we have to provide arguments for the four parameters.

The first parameter is the confusion matrix stored as a table.

The second and third parameters are the names of the positive and negative labels. The example used yes and no, so those are our input arguments.

If you have generated a confusion matrix with the predictions as the rows and truth labels as the columns then change the fourth argument to 'col'. Our truth labels are on the rows, so 'row' is specified.

```{r}
table_metrics(confusion, 'yes', 'no', 'row')
```

The function returns a list with the confusion matrix rates/metrics. You can save the list and subset for the rate/metric you are interested in.

```{r}
my_metrics <- table_metrics(confusion, 'yes', 'no', 'row')
my_metrics$sensitivity
```

Finally, if you want more significant digits (default is set to 3), supply it as the fifth argument.

I have some additional notes on [machine learning evaluation](https://github.com/davetang/machine_learning/tree/main/evaluation) that may also be of interest. And that's it!

## F1 score

Generate labels.

```{r true_label}
true_label <- factor(c(rep(1, 80), rep(2, 10), rep(3, 10)), levels = 1:3)
true_label
```

Predictions.

```{r pred_label}
pred_label <- factor(c(2, 3, rep(1, 98)), levels = 1:3)
pred_label
```

Generate confusion matrix.

```{r cm}
cm <- table(truth = true_label, predict = pred_label)
cm
```
Using [yardstick::f_meas](https://yardstick.tidymodels.org/reference/f_meas.html).

```{r f_meas}
if(!require("yardstick")){
  install.packages("yardstick")
}

yardstick::f_meas(cm)
```

Using `f_meas_vec()`.

```{r f_meas_vec}
yardstick::f_meas_vec(truth = true_label, estimate = pred_label)
```

High accuracy but low $F_1$.

```{r}
yardstick::accuracy(cm)
```

## Compare calculations

Double check to see if `table_metrics()` calculations are correct.

```{r testing}
true_label <- factor(c(rep(1, 90), rep(2, 10)), levels = 1:2)
pred_label <- factor(rep(1, 100), levels = 1:2)

cm <- table(truth = true_label, predict = pred_label)
cm
```

Calculate metrics.

```{r cm_metrics}
cm_metrics <- table_metrics(cm, 1, 2, 'row')
```

Using {yardstick}.

```{r}
cm_metrics$accuracy
yardstick::accuracy(cm)$.estimate
```

F1 score.

```{r}
cm_metrics$f1_score
yardstick::f_meas(cm)$.estimate
```

Specificity.

```{r}
cm_metrics$specificity
yardstick::specificity(cm)$.estimate
```

Note the difference in sensitivity; this is because the function expects that **the true class results should be in the columns of the table** and we have it the other way around.

```{r}
cm_metrics$recall
yardstick::recall(cm)$.estimate
yardstick::sensitivity(cm)$.estimate
```

If we provide the labels manually, the sensitivity is calculated properly.

```{r}
yardstick::sensitivity_vec(true_label, pred_label)
```

Same for precision.

```{r}
cm_metrics$precision
yardstick::precision_vec(true_label, pred_label)
```
