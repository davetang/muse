---
title: "Information"
date: "`r Sys.Date()`"
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(mpmi))
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

According to [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), information is present whenever a signal is transmitted from one place (sender) to another (receiver).

Information theory, founded by Shannon, studies the **quantification, transmission, storage, and processing of information**. At its core, it answers:

* How much *uncertainty* does a random variable have?
* How much information is gained when we learn the outcome of something uncertain?
* How efficiently can we transmit data over noisy communication channels?

Key concepts include:

* *Entropy (H)*: A widely used measure ([Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))) of uncertainty in a random variable. Higher entropy means more unpredictability.
* *Joint entropy & conditional entropy*: Extensions that measure combined or conditional uncertainties.
* *Channel capacity*: The maximum rate at which information can be transmitted over a noisy channel with arbitrarily low error.

### Bits

Information theory quantifies uncertainty using bits (binary digits) as the unit of measurement. One bit represents the amount of information needed to resolve a binary choice, such as answering a single yes/no question. This binary framework serves as the fundamental building block because any complex decision can be decomposed into a series of binary choices.

For example, consider flipping a coin. Before the flip, there are two equally likely outcomes: heads or tails. This uncertainty can be resolved with a single binary question: "Is it heads?" Once you observe the result, this question is answered, and all uncertainty is eliminated. Since resolving this uncertainty required one binary question, the coin flip provides exactly 1 bit of information.

In general:

* Resolving between 2 equally likely options = 1 bit
* Resolving between 4 equally likely options = 2 bits
* Resolving between 8 equally likely options = 3 bits
* Resolving between N equally likely options = log2(N) bits

```{r bit_eg}
log2(2)
log2(4)
log2(8)
```

### Mutual Information

[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) between two random variables $X$ and $Y$ measures how much knowing one reduces uncertainty about the other.

$$
I(X;Y) = H(X) + H(Y) - H(X, Y)
$$

or equivalently,

$$
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X).
$$

If $I(X;Y) = 0$, $X$ and $Y$ are independent (no shared information); larger values mean stronger statistical dependence.

## mpmi

The [Mixed-Pair Mutual Information Estimators](https://cran.r-project.org/web/packages/mpmi/index.html) {mpmi} package:

> Uses a kernel smoothing approach to calculate Mutual Information for comparisons between all types of variables including continuous vs continuous, continuous vs discrete and discrete vs discrete. Uses a nonparametric bias correction giving Bias Corrected Mutual Information (BCMI). Implemented efficiently in Fortran 95 with OpenMP and suited to large genomic datasets. 

```{r install_dep, eval=FALSE}
install.packages('mpmi')
```

The `dmi()` function calculates MI and BCMI between a set of discrete variables held as columns in a matrix. It also performs jackknife bias correction and provides a z-score for the hypothesis of no association. Also included are the `*.pw` functions that calculate MI between two vectors only. The `*njk` functions do not perform the jackknife and are therefore faster.

MI quantifies the reduction in uncertainty about one variable given knowledge of another. It's measured in bits (or nats, depending on the logarithm base) and ranges from 0 (variables are independent) to min(H(X), H(Y)) where H is entropy. Unlike correlation, MI captures non-linear relationships and works naturally with categorical data.

MI estimates from finite samples are positively biased; they tend to overestimate the true MI. The jackknife procedure systematically removes each observation, recalculates MI, and uses these values to estimate and subtract the bias. This is particularly important for small sample sizes or sparse contingency tables.

The results of `dmi()` are in many ways similar to a correlation matrix, with each row and column index corresponding to a given variable.

## Examples

### mtcars

Exploring a group of categorical variables (from the examples in the documentation of the `dmi()` function).

* `cyl` - Number of cylinders
* `vs` - Engine (0 = V-shaped, 1 = straight)
* `am` - Transmission (0 = automatic, 1 = manual)
* `gear` - Number of forward gears
* `carb` - Number of carburetors (a device used by a gasoline internal combustion engine to control and mix air and fuel entering the engine)

```{r discrete_example}
my_vars <- c("cyl","vs","am","gear","carb")
dat <- mtcars[, my_vars]
discresults <- dmi(dat)

add_names <- function(res, names){
  purrr::map(res, \(x){
    row.names(x) <- names
    colnames(x) <- names
    x
  })
}

add_names(discresults, my_vars)
```

Each matrix is symmetric (5×5), where rows and columns represent the same variables in the same order. The diagonal represents each variable with itself.

* `$mi` is the raw mutual information and these are the uncorrected MI values in bits.

* Diagonal (e.g., `cyl` with itself = 1.06): This is the entropy of each variable—how much uncertainty/information it contains
- Off-diagonal (e.g., `cyl-vs` = 0.43): How much information they share

* `cyl-carb` (0.51): Knowing cylinders tells you a lot about carburetors
* `cyl-vs` (0.43): Cylinders and engine shape are related
* `gear-am` (0.44): Gears and transmission type are connected
* `vs-am` (0.014): Engine shape and transmission are nearly independent

* `$bcmi` is the bias-corrected MI; after the jackknife correction:

* Values are generally similar to raw MI
* `vs-am` = -0.003: The negative value indicates the raw MI was entirely due to sampling bias; these variables are essentially independent
* The correction is more pronounced for weaker associations

* `$zvalues` show the statistical significance

The null hypothesis is that there is no association between variables.

Rule of thumb:

* |z| > 1.96 suggests significance at alpha = 0.05
* |z| > 2.58 suggests significance at alpha = 0.01

Highly significant associations (|z| > 3) include:

* `cyl-carb` (z = 7.47): Strong evidence cylinders and carburetors are related
* `cyl-vs` (z = 3.39): Cylinders and engine shape are associated
* `gear-am` (z = 5.52): Gears and transmission are related
* `vs-am` (z = -0.10): Confirms independence
* `cyl-am` (z = 1.06): Weak/no association

Cars with more cylinders tend to have more carburetors and V-shaped engines. Manual transmissions are associated with different gear configurations but engine shape doesn't predict transmission type.

### Random

Two random variables.

```{r eg2}
set.seed(1984)
n <- 1000
X <- rbinom(n, 1, 0.5)
Y <- rbinom(n, 1, 0.5)
xy <- c('X', 'Y')

my_mat <- matrix(data = c(X,Y), nrow = n)
add_names(dmi(my_mat), xy)
```

80% of the time make Y the same as X; the other 20% of the time make Y 1 less than X.

```{r eg3}
set.seed(1984)
n <- 1000
X <- rbinom(n, 1, 0.5)
Y <- ifelse(runif(n) < 0.8, X, 1 - X)
xy <- c('X', 'Y')

my_mat <- matrix(data = c(X,Y), nrow = n)
add_names(dmi(my_mat), xy)
```
