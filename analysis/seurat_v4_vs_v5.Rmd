---
title: "Seurat v4 versus v5"
date: "`r Sys.Date()`"
output:
  workflowr::wflow_html:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# https://stackoverflow.com/questions/30237310/setting-work-directory-in-knitr-using-opts-chunksetroot-dir-doesnt-wor
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
library(tidyverse)
library(Seurat)
```

## sctransform

The paper [Comparison and evaluation of statistical error models for scRNA-seq](https://link.springer.com/article/10.1186/s13059-021-02584-9) is the basis for the default approach used in Seurat version 5. The following is text from the paper:

* Heterogeneity in single-cell RNA-seq (scRNA-seq) data is driven by multiple sources, including biological variation in cellular state as well as technical variation introduced during experimental processing. Deconvolving these effects is a key challenge for preprocessing workflows.
    * Separating biological heterogeneity across cells that corresponds to differences in cell type and state from alternative sources of variation represents a key analytical challenge in the normalization and preprocessing of single-cell RNA-seq data.
* Data normalization aims to adjust for differences in cellular sequencing depth, which collectively arise from fluctuations in cellular RNA content, efficiency in lysis and reverse transcription, and stochastic sampling during next-generation sequencing.
* **Variance stabilization aims to address the confounding relationship between gene abundance and gene variance, and to ensure that both lowly and highly expressed genes can contribute to the downstream definition of cellular state**.

Using statistical models like Generalised Linear Models:

* Two recent studies proposed to use generalized linear models (GLMs), where cellular sequencing depth was included as a covariate, as part of scRNA-seq preprocessing workflows.
* The sctransform approach utilizes the **Pearson residuals from negative binomial regression as input to standard dimensional reduction techniques**, while GLM-PCA focuses on a generalized version of principal component analysis (PCA) for data with Poisson-distributed errors.
* More broadly, multiple techniques aim to learn a latent state that captures biologically relevant cellular heterogeneity using either matrix factorization or neural networks, alongside a defined error model that describes the variation that is not captured by the latent space.

If a regression model doesn't fully explain variability, the residuals might contain structure that another technique can capture to uncover hidden patterns. For example, if a regression model captures main trends, applying Principal Component Analysis (PCA) on residuals can find underlying structures in the unexplained variance. Another use case can be clustering on residuals to group data points based on deviations from a model.

Parameterising statistical models:

* Likelihood-based approaches require an explicit definition of a statistical error model for scRNA-seq, and there is little consensus on how to define or parameterize this model.
* Multiple groups have utilized a Poisson error model but others argue that the data exhibit evidence of overdispersion, requiring the use of a negative-binomial (NB) distribution.
* Methods that assume a NB distribution have different methods to parameterize their model.
    * A recent study argued that fixing the NB inverse overdispersion parameter $\theta$ to a single value is an appropriate estimate of technical overdispersion for all genes in all scRNA-seq datasets, while others propose learning unique parameter values for each gene in each dataset.
* This lack of consensus is further exemplified by the scvi-tools suite, which supports nine different methods for parameterizing error models.
* The purpose of this error model is to describe and quantify heterogeneity that is not captured by biologically relevant differences in cell state, and highlights a specific question: **How can we model the observed variation in gene expression for an scRNA-seq experiment conducted on a biologically "homogeneous" population?**

## Error and variance

* Error modeling refers to capturing and understanding the **uncertainty, randomness, and deviations** in data or predictions. Error is not exactly synonymous with variance but they are related.
* Errors arise from:
    * Randomness: unavoidable variability in data, e.g., measurement noise and natural fluctuations
    * Model Imperfections: due to missing information or incorrect model assumptions.
* Statistical models, like regression, often include an error term $\epsilon$ to account for these uncertainties:

$$
Y = f(X) + \epsilon
$$

where $\epsilon$ captures random fluctuations or unknown influences.

While error contributes to variance, they are distinct:

* **Error**: The deviation of an observation from the model's predicted value.
* **Variance**: A statistical measure of how much values deviate from their mean (spread of data).

Errors can be random (causing variability) or systematic (bias), but variance is a quantification of dispersion.

* In regression, we separate variance into **explained variance** (by the model) and **unexplained variance** (error).
* Error variance $\sigma^2$ is modeled to improve predictions and uncertainty estimation.
* For a negative binomial regression model, the **Pearson residual** $r_i$ for observation $i$ is given by:  

$$
r_i = \frac{y_i - \hat{y}_i}{\sqrt{\text{Var}(y_i)}}
$$

where:  
* $y_i$ = observed count  
* $\hat{y}_i$ = predicted mean (expected value under the model)  
* $\text{Var}(y_i)$ = model-estimated variance of $y_i$

* Unlike Poisson regression (where $\text{Var}(y) = \hat{y}$), the **negative binomial** model accounts for **overdispersion** using a dispersion parameter $\theta$, and the variance is:  

$$
\text{Var}(y_i) = \hat{y}_i + \frac{\hat{y}_i^2}{\theta}
$$

This means the variance **grows faster than the mean**, making negative binomial regression suitable when count data has extra variability.

* Pearson residuals can be used for:
    * **Standardized measure** - Large residuals ($|r_i| > 2$) may indicate poor model fit.
    * **Overdispersion check** - If Pearson residuals systematically increase with predicted values, it suggests the model may not fully account for overdispersion.
    * **Model diagnostics** - Residual plots help assess whether assumptions (e.g., correct functional form) hold.

Pearson residuals focus on **variance-adjusted differences** and deviance residuals come from likelihood-based goodness-of-fit measures. They both help diagnose model fit, but deviance residuals tend to emphasise extreme deviations more. Pearson residuals in negative binomial regression are useful for model diagnostics, particularly for checking overdispersion and assessing fit.

## Seurat object

Import [raw pbmc3k dataset](pbmc3k.html) from my server.

```{r seurat_obj}
seurat_obj <- readRDS(url("https://davetang.org/file/pbmc3k_seurat.rds", "rb"))
seurat_obj
```

Filter.

```{r pbmc3k}
pbmc3k <- CreateSeuratObject(
  counts = seurat_obj@assays$RNA$counts,
  min.cells = 3,
  min.features = 200,
  project = "pbmc3k"
)
pbmc3k
```

## Seurat workflows

Process with the Seurat 4 workflow.

```{r seurat_wf_v4}
seurat_wf_v4 <- function(seurat_obj, scale_factor = 1e4, num_features = 2000, num_pcs = 30, cluster_res = 0.5, debug_flag = FALSE){
  
  seurat_obj <- NormalizeData(seurat_obj, normalization.method = "LogNormalize", scale.factor = scale_factor, verbose = debug_flag)
  seurat_obj <- FindVariableFeatures(seurat_obj, selection.method = 'vst', nfeatures = num_features, verbose = debug_flag)
  seurat_obj <- ScaleData(seurat_obj, verbose = debug_flag)
  seurat_obj <- RunPCA(seurat_obj, verbose = debug_flag)
  seurat_obj <- RunUMAP(seurat_obj, dims = 1:num_pcs, verbose = debug_flag)
  seurat_obj <- FindNeighbors(seurat_obj, dims = 1:num_pcs, verbose = debug_flag)
  seurat_obj <- FindClusters(seurat_obj, resolution = cluster_res, verbose = debug_flag)
  
  seurat_obj
}

pbmc3k_v4 <- seurat_wf_v4(pbmc3k)
pbmc3k_v4
```

UMAP.

```{r umap_v4}
DimPlot(pbmc3k_v4, reduction = "umap")
```


```{r seurat_wf_v5}
seurat_wf_v5 <- function(seurat_obj, scale_factor = 1e4, num_features = 2000, num_pcs = 30, cluster_res = 0.5, debug_flag = FALSE){
  
  seurat_obj <- SCTransform(seurat_obj, verbose = debug_flag)
  seurat_obj <- RunPCA(seurat_obj, verbose = debug_flag)
  seurat_obj <- RunUMAP(seurat_obj, dims = 1:num_pcs, verbose = debug_flag)
  seurat_obj <- FindNeighbors(seurat_obj, dims = 1:num_pcs, verbose = debug_flag)
  seurat_obj <- FindClusters(seurat_obj, resolution = cluster_res, verbose = debug_flag)
  
  seurat_obj
}

pbmc3k_v5 <- seurat_wf_v5(pbmc3k)
pbmc3k_v5
```

UMAP.

```{r umap_v5}
DimPlot(pbmc3k_v5, reduction = "umap")
```

### Data layer

Version 4 store log normalised data.

```{r pbmc3k_v4_data}
colSums(pbmc3k_v4@assays$RNA$data)[1:6]
```

The data layer is in the SCT assay.

```{r pbmc3k_v5_data}
colSums(pbmc3k_v5@assays$SCT$data)[1:6]
```

### Compare clustering

More granular clustering of version 4's cluster 0 in version 5.

```{r compare_clustering}
stopifnot(all(row.names(pbmc3k_v4@meta.data) == row.names(pbmc3k_v5@meta.data)))

table(
  pbmc3k_v4@meta.data$seurat_clusters,
  pbmc3k_v5@meta.data$seurat_clusters
)
```
